{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahraniayudyaa/finnalterm-ml/blob/main/UAS_ML_CNN_ZAHRANI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPLEMENTASI END-TO-END CNN UNTUK KLASIFIKASI GAMBAR**"
      ],
      "metadata": {
        "id": "CM-p1989nHoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Install Requirements**"
      ],
      "metadata": {
        "id": "_GAbYfCymywR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VxHQaUqkNrf",
        "outputId": "6de2f201-ee38-46e5-bb20-75406edef445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow numpy pandas matplotlib seaborn opencv-python scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Memuat Dataset**"
      ],
      "metadata": {
        "id": "VnGRelECq8vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# FUNGSI UNTUK MEMUAT DATASET DARI GOOGLE DRIVE\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import zipfile\n",
        "import gdown\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def load_dataset_from_gdrive_colab(drive_url=\"https://drive.google.com/drive/folders/15hDCm5WXIcBCOm0CVue-pbgOEhA0BoLt\"):\n",
        "    \"\"\"\n",
        "    Fungsi untuk memuat dataset dari Google Drive di Google Colab\n",
        "    \"\"\"\n",
        "    print(\"Loading dataset from Google Drive...\")\n",
        "\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Dataset mungkin sudah ada di Google Drive user\n",
        "    # Atau kita perlu download dari link yang diberikan\n",
        "\n",
        "    # Method 1: Jika dataset sudah di-share secara publik\n",
        "    try:\n",
        "        # Install gdown jika belum ada\n",
        "        !pip install -q gdown\n",
        "\n",
        "        # Download dataset sebagai zip (jika tersedia sebagai zip)\n",
        "        # Ganti FILE_ID dengan ID dari Google Drive\n",
        "        # Untuk mendapatkan FILE_ID:\n",
        "        # - Buka file di Google Drive\n",
        "        # - URL akan seperti: https://drive.google.com/file/d/FILE_ID/view\n",
        "        file_id = \"15hDCm5WXIcBCOm0CVue-pbgOEhA0BoLt\"  # ID dari link yang diberikan\n",
        "\n",
        "        # Coba download sebagai zip\n",
        "        !gdown --id $file_id --output dataset.zip\n",
        "\n",
        "        # Extract zip\n",
        "        with zipfile.ZipFile('dataset.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall('/content/dataset')\n",
        "\n",
        "        dataset_path = '/content/dataset'\n",
        "\n",
        "    except:\n",
        "        # Method 2: Jika dataset dalam folder dengan struktur tertentu\n",
        "        print(\"Mencoba akses langsung ke folder...\")\n",
        "\n",
        "        # Untuk folder, kita perlu membuat direktori struktur manual\n",
        "        dataset_path = '/content/drive/MyDrive/fish_dataset'  # Ganti dengan path yang sesuai\n",
        "\n",
        "        # Jika tidak ada, coba cari\n",
        "        if not os.path.exists(dataset_path):\n",
        "            print(\"Mencari dataset di Google Drive...\")\n",
        "            # Cari folder dengan pattern tertentu\n",
        "            import subprocess\n",
        "\n",
        "            # Cari file yang mengandung 'fish' atau 'dataset'\n",
        "            result = subprocess.run(['find', '/content/drive', '-type', 'd', '-name', '*fish*', '-o', '-name', '*dataset*'],\n",
        "                                  capture_output=True, text=True)\n",
        "\n",
        "            if result.stdout:\n",
        "                possible_paths = result.stdout.strip().split('\\n')\n",
        "                dataset_path = possible_paths[0] if possible_paths else None\n",
        "                print(f\"Found possible dataset at: {dataset_path}\")\n",
        "\n",
        "    return dataset_path\n",
        "\n",
        "# ============================================\n",
        "# FUNGSI UTAMA UNTUK MEMUAT SELURUH DATASET\n",
        "# ============================================\n",
        "\n",
        "def load_fish_dataset(dataset_path=None, img_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Fungsi utama untuk memuat dataset ikan dengan struktur train/test/val\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"MEMUAT DATASET IKAN\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Jika path tidak diberikan, coba load dari Google Drive\n",
        "    if dataset_path is None:\n",
        "        # Cek apakah di Colab\n",
        "        try:\n",
        "            import google.colab\n",
        "            in_colab = True\n",
        "        except:\n",
        "            in_colab = False\n",
        "\n",
        "        if in_colab:\n",
        "            print(\"Detected Google Colab environment\")\n",
        "            dataset_path = load_dataset_from_gdrive_colab()\n",
        "        else:\n",
        "            print(\"Detected local environment\")\n",
        "            dataset_path = load_dataset_from_gdrive_local()\n",
        "\n",
        "    # Pastikan dataset_path ada\n",
        "    if not os.path.exists(dataset_path):\n",
        "        # Coba cari dengan nama umum\n",
        "        possible_paths = [\n",
        "            dataset_path,\n",
        "            \"fish_dataset\",\n",
        "            \"/content/drive/MyDrive/fish_dataset\",\n",
        "            \"/content/fish_dataset\",\n",
        "            \"./fish_dataset\"\n",
        "        ]\n",
        "\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                dataset_path = path\n",
        "                break\n",
        "\n",
        "    print(f\"Dataset path: {dataset_path}\")\n",
        "\n",
        "    # Cek struktur folder\n",
        "    print(\"\\nChecking folder structure...\")\n",
        "    train_path = os.path.join(dataset_path, 'train')\n",
        "    test_path = os.path.join(dataset_path, 'test')\n",
        "    val_path = os.path.join(dataset_path, 'val')\n",
        "\n",
        "    # Jika tidak ada struktur train/test/val, mungkin dataset dalam format lain\n",
        "    if not os.path.exists(train_path):\n",
        "        print(\"Struktur train/test/val tidak ditemukan.\")\n",
        "        print(\"Mencoba format alternatif...\")\n",
        "\n",
        "        # Cek apakah langsung berisi folder kelas\n",
        "        subdirs = [d for d in os.listdir(dataset_path)\n",
        "                  if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "\n",
        "        if len(subdirs) > 0:\n",
        "            print(f\"Ditemukan {len(subdirs)} subdirectories: {subdirs[:5]}...\")\n",
        "\n",
        "            # Split manual menjadi train/test/val\n",
        "            return load_and_split_dataset(dataset_path, img_size)\n",
        "        else:\n",
        "            raise ValueError(f\"Struktur dataset tidak valid di: {dataset_path}\")\n",
        "\n",
        "    # Load data dari masing-masing folder\n",
        "    print(\"\\nLoading training data...\")\n",
        "    X_train, y_train, class_names = load_images_from_folder(train_path, img_size)\n",
        "\n",
        "    print(\"Loading test data...\")\n",
        "    X_test, y_test, _ = load_images_from_folder(test_path, img_size)\n",
        "\n",
        "    print(\"Loading validation data...\")\n",
        "    X_val, y_val, _ = load_images_from_folder(val_path, img_size)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DATASET SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Classes: {class_names}\")\n",
        "    print(f\"Number of classes: {len(class_names)}\")\n",
        "    print(f\"Training samples: {len(X_train)}\")\n",
        "    print(f\"Test samples: {len(X_test)}\")\n",
        "    print(f\"Validation samples: {len(X_val)}\")\n",
        "    print(f\"Image shape: {X_train[0].shape}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return (X_train, y_train), (X_test, y_test), (X_val, y_val), class_names"
      ],
      "metadata": {
        "id": "yfUTzGBGpIga"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Import Library**"
      ],
      "metadata": {
        "id": "NO7kL9bUrDZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPOR LIBRARY YANG DIPERLUKAN\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Library untuk deep learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, applications\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "\n",
        "# Library untuk evaluasi\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Set random seed untuk reprodusibilitas\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Versi TensorFlow:\", tf.__version__)\n",
        "print(\"Versi Keras:\", keras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3JsDLbXnCqi",
        "outputId": "82ce0d25-dc78-45bf-d402-3a5c256cba87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versi TensorFlow: 2.19.0\n",
            "Versi Keras: 3.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Load Data**"
      ],
      "metadata": {
        "id": "SmRcBMWxrF3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LOAD DAN EKSPLORASI DATA\n",
        "# ============================================\n",
        "\n",
        "\n",
        "def load_and_explore_data():\n",
        "    \"\"\"\n",
        "    Fungsi untuk memuat dan mengeksplorasi dataset\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EKSPLORASI DATASET\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Shape training data: {X_train.shape}\")\n",
        "    print(f\"Shape training labels: {y_train.shape}\")\n",
        "    print(f\"Shape test data: {x_test.shape}\")\n",
        "    print(f\"Shape test labels: {y_test.shape}\")\n",
        "\n",
        "    # Tampilkan informasi kelas\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    print(f\"\\nJumlah kelas: {num_classes}\")\n",
        "\n",
        "    # Hitung jumlah sampel per kelas\n",
        "    unique, counts = np.unique(y_train, return_counts=True)\n",
        "    class_distribution = dict(zip(unique, counts))\n",
        "    print(\"\\nDistribusi kelas pada data training:\")\n",
        "    for cls, count in class_distribution.items():\n",
        "        print(f\"  Kelas {cls}: {count} sampel\")\n",
        "\n",
        "    # Visualisasi distribusi kelas\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(class_distribution.keys(), class_distribution.values())\n",
        "    plt.title('Distribusi Kelas pada Data Training')\n",
        "    plt.xlabel('Kelas')\n",
        "    plt.ylabel('Jumlah Sampel')\n",
        "    plt.show()\n",
        "\n",
        "    return (X_train, y_train), (x_test, y_test), num_classes\n",
        "\n",
        "# Load data\n",
        "(X_train, y_train), (x_test, y_test), num_classes = load_and_explore_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "keobdKjUnYEi",
        "outputId": "acf1de56-d066-4043-c39f-4a2a3df1cea0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "EKSPLORASI DATASET\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1566894964.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_explore_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1566894964.py\u001b[0m in \u001b[0;36mload_and_explore_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape training data: {X_train.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape training labels: {y_train.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape test data: {x_test.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Visualisasi Sampel**"
      ],
      "metadata": {
        "id": "TalxQUvxrOr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# VISUALISASI SAMPEL GAMBAR\n",
        "# ============================================\n",
        "\n",
        "def visualize_samples(images, labels, class_names=None, num_samples=10):\n",
        "    \"\"\"\n",
        "    Fungsi untuk menampilkan sampel gambar dari dataset\n",
        "    \"\"\"\n",
        "    if class_names is None:\n",
        "        class_names = [f'Class {i}' for i in range(num_classes)]\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    for i in range(num_samples):\n",
        "        idx = random.randint(0, len(images)-1)\n",
        "        plt.subplot(2, 5, i+1)\n",
        "        plt.imshow(images[idx])\n",
        "        plt.title(f'Label: {class_names[labels[idx][0]]}')\n",
        "        plt.axis('off')\n",
        "    plt.suptitle('Contoh Sampel Gambar dari Dataset', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Tampilkan contoh gambar\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "visualize_samples(x_train, y_train, class_names)"
      ],
      "metadata": {
        "id": "cOhtrkvdnaAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Preprocessing Data**"
      ],
      "metadata": {
        "id": "EcMRSdsjrbUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PREPROCESSING DATA\n",
        "# ============================================\n",
        "\n",
        "def preprocess_data(x_train, x_test, y_train, y_test, num_classes):\n",
        "    \"\"\"\n",
        "    Fungsi untuk preprocessing data\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PREPROCESSING DATA\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Normalisasi pixel values ke range [0, 1]\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "    # One-hot encoding untuk label\n",
        "    y_train_onehot = to_categorical(y_train, num_classes)\n",
        "    y_test_onehot = to_categorical(y_test, num_classes)\n",
        "\n",
        "    # Split data training menjadi training dan validation\n",
        "    x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n",
        "        x_train, y_train_onehot, test_size=0.2, random_state=42, stratify=y_train\n",
        "    )\n",
        "\n",
        "    print(f\"Data training shape: {x_train_split.shape}\")\n",
        "    print(f\"Data validation shape: {x_val_split.shape}\")\n",
        "    print(f\"Data test shape: {x_test.shape}\")\n",
        "\n",
        "    return x_train_split, x_val_split, x_test, y_train_split, y_val_split, y_test_onehot\n",
        "\n",
        "# Preprocess data\n",
        "x_train_split, x_val_split, x_test_preprocessed, y_train_split, y_val_split, y_test_onehot = preprocess_data(\n",
        "    x_train, x_test, y_train, y_test, num_classes\n",
        ")"
      ],
      "metadata": {
        "id": "OzkLATA7ncbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Augmentasi Data**"
      ],
      "metadata": {
        "id": "FjstSQXerh-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# AUGMENTASI DATA\n",
        "# ============================================\n",
        "\n",
        "def create_augmentation_generator():\n",
        "    \"\"\"\n",
        "    Fungsi untuk membuat generator augmentasi data\n",
        "    \"\"\"\n",
        "    print(\"\\nMembuat generator augmentasi data...\")\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=20,           # Rotasi acak hingga 20 derajat\n",
        "        width_shift_range=0.2,       # Pergeseran horizontal\n",
        "        height_shift_range=0.2,      # Pergeseran vertikal\n",
        "        horizontal_flip=True,        # Flip horizontal\n",
        "        zoom_range=0.2,              # Zoom acak\n",
        "        shear_range=0.2,             # Shear transformation\n",
        "        fill_mode='nearest'          # Mengisi pixel yang hilang\n",
        "    )\n",
        "\n",
        "    return datagen\n",
        "\n",
        "# Buat generator augmentasi\n",
        "train_datagen = create_augmentation_generator()\n",
        "val_datagen = ImageDataGenerator()  # Tidak augmentasi untuk validation"
      ],
      "metadata": {
        "id": "GAhmH097nerG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# HANDLING CLASS IMBALANCE\n",
        "# ============================================\n",
        "\n",
        "def handle_class_imbalance(y_train):\n",
        "    \"\"\"\n",
        "    Fungsi untuk menghitung class weights untuk menangani imbalance\n",
        "    \"\"\"\n",
        "    # Flatten labels\n",
        "    y_train_flat = y_train.flatten()\n",
        "\n",
        "    # Hitung class weights\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_train_flat),\n",
        "        y=y_train_flat\n",
        "    )\n",
        "\n",
        "    # Konversi ke dictionary\n",
        "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CLASS WEIGHTS UNTUK HANDLE IMBALANCE\")\n",
        "    print(\"=\"*50)\n",
        "    for cls, weight in class_weight_dict.items():\n",
        "        print(f\"Kelas {cls}: weight = {weight:.3f}\")\n",
        "\n",
        "    return class_weight_dict\n",
        "\n",
        "# Hitung class weights\n",
        "class_weight_dict = handle_class_imbalance(y_train)"
      ],
      "metadata": {
        "id": "7Sakcm83ngw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Membangun Model CNN**"
      ],
      "metadata": {
        "id": "cUzqMDhMrpfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# MEMBANGUN MODEL CNN DARI SCRATCH\n",
        "# ============================================\n",
        "\n",
        "def build_cnn_model(input_shape=(32, 32, 3), num_classes=10):\n",
        "    \"\"\"\n",
        "    Membangun model CNN dari scratch\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MEMBANGUN MODEL CNN DARI SCRATCH\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    model = Sequential([\n",
        "        # Convolutional Layer 1\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Convolutional Layer 2\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Convolutional Layer 3\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', 'AUC']\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# Build CNN model\n",
        "cnn_model = build_cnn_model(input_shape=x_train_split.shape[1:], num_classes=num_classes)"
      ],
      "metadata": {
        "id": "VjbpjNWdni9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# TRANSFER LEARNING DENGAN PRE-TRAINED MODEL\n",
        "# ============================================\n",
        "\n",
        "def build_transfer_learning_model(input_shape=(32, 32, 3), num_classes=10):\n",
        "    \"\"\"\n",
        "    Membangun model menggunakan transfer learning dengan VGG16\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRANSFER LEARNING DENGAN VGG16\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Load pre-trained VGG16 tanpa top layer\n",
        "    base_model = applications.VGG16(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "\n",
        "    # Freeze base model layers\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Build custom top layers\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "    # Preprocessing sesuai dengan VGG16\n",
        "    x = applications.vgg16.preprocess_input(inputs)\n",
        "    x = base_model(x, training=False)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', 'AUC']\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    return model, base_model\n",
        "\n",
        "# Build transfer learning model\n",
        "tl_model, base_model = build_transfer_learning_model(\n",
        "    input_shape=(32, 32, 3),  # Resize sesuai kebutuhan\n",
        "    num_classes=num_classes\n",
        ")"
      ],
      "metadata": {
        "id": "w9ucKlHGnlnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CALLBACKS UNTUK TRAINING\n",
        "# ============================================\n",
        "\n",
        "def create_callbacks(model_name='best_model.h5'):\n",
        "    \"\"\"\n",
        "    Membuat callbacks untuk training\n",
        "    \"\"\"\n",
        "    callbacks = [\n",
        "        # Early stopping jika tidak ada improvement\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "\n",
        "        # Reduce learning rate jika plateau\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-6,\n",
        "            verbose=1\n",
        "        ),\n",
        "\n",
        "        # Save best model\n",
        "        ModelCheckpoint(\n",
        "            model_name,\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            mode='max',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    return callbacks"
      ],
      "metadata": {
        "id": "ceCvWTXHnocC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. Training Model**"
      ],
      "metadata": {
        "id": "N6mdO19mrzva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# TRAINING MODEL CNN\n",
        "# ============================================\n",
        "\n",
        "def train_model(model, x_train, y_train, x_val, y_val,\n",
        "                class_weight=None, epochs=50, batch_size=64, model_name='cnn'):\n",
        "    \"\"\"\n",
        "    Fungsi untuk training model\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\"*50)\n",
        "    print(f\"TRAINING MODEL {model_name.upper()}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Buat callbacks\n",
        "    callbacks = create_callbacks(f\"{model_name}_best.h5\")\n",
        "\n",
        "    # Training dengan augmentasi\n",
        "    train_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)\n",
        "\n",
        "    # Training model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=len(x_train) // batch_size,\n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=callbacks,\n",
        "        class_weight=class_weight,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "# Training CNN model\n",
        "print(\"\\nTraining CNN model dari scratch...\")\n",
        "cnn_history = train_model(\n",
        "    cnn_model,\n",
        "    x_train_split, y_train_split,\n",
        "    x_val_split, y_val_split,\n",
        "    class_weight=class_weight_dict,\n",
        "    epochs=50,\n",
        "    model_name='cnn_from_scratch'\n",
        ")\n",
        "\n",
        "# Training Transfer Learning model\n",
        "print(\"\\nTraining Transfer Learning model...\")\n",
        "tl_history = train_model(\n",
        "    tl_model,\n",
        "    x_train_split, y_train_split,\n",
        "    x_val_split, y_val_split,\n",
        "    class_weight=class_weight_dict,\n",
        "    epochs=30,\n",
        "    model_name='transfer_learning'\n",
        ")"
      ],
      "metadata": {
        "id": "RW7JHCl-nqoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10. Fine-Tuning Model**"
      ],
      "metadata": {
        "id": "_9tv1Uucr6Yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 11. FINE-TUNING TRANSFER LEARNING MODEL\n",
        "# ============================================\n",
        "\n",
        "def fine_tune_model(model, base_model, x_train, y_train, x_val, y_val):\n",
        "    \"\"\"\n",
        "    Fine-tuning untuk transfer learning model\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINE-TUNING TRANSFER LEARNING MODEL\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Unfreeze beberapa layer terakhir\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # Freeze layer awal, unfreeze layer akhir\n",
        "    for layer in base_model.layers[:15]:\n",
        "        layer.trainable = False\n",
        "    for layer in base_model.layers[15:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # Recompile dengan learning rate lebih rendah\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-5),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', 'AUC']\n",
        "    )\n",
        "\n",
        "    # Training untuk fine-tuning\n",
        "    callbacks = create_callbacks(\"fine_tuned_best.h5\")\n",
        "\n",
        "    history_finetune = model.fit(\n",
        "        x_train, y_train,\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return history_finetune\n",
        "\n",
        "# Fine-tuning\n",
        "finetune_history = fine_tune_model(\n",
        "    tl_model, base_model,\n",
        "    x_train_split, y_train_split,\n",
        "    x_val_split, y_val_split\n",
        ")"
      ],
      "metadata": {
        "id": "82NGzTiIns-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **11. Evaluasi Model**"
      ],
      "metadata": {
        "id": "a8waMIoxr-oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# EVALUASI MODEL\n",
        "# ============================================\n",
        "\n",
        "def evaluate_model(model, x_test, y_test, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Fungsi untuk evaluasi model\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\"*50)\n",
        "    print(f\"EVALUASI {model_name.upper()}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Predict\n",
        "    y_pred_proba = model.predict(x_test)\n",
        "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"Akurasi: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names[:num_classes]))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names[:num_classes],\n",
        "                yticklabels=class_names[:num_classes])\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return accuracy, y_pred\n",
        "\n",
        "# Evaluate semua model\n",
        "print(\"\\nEvaluasi CNN dari scratch...\")\n",
        "cnn_accuracy, cnn_predictions = evaluate_model(cnn_model, x_test_preprocessed, y_test_onehot, \"CNN dari Scratch\")\n",
        "\n",
        "print(\"\\nEvaluasi Transfer Learning model...\")\n",
        "tl_accuracy, tl_predictions = evaluate_model(tl_model, x_test_preprocessed, y_test_onehot, \"Transfer Learning VGG16\")"
      ],
      "metadata": {
        "id": "EYGZYnf_nwOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12. Visualisasi Hasil**"
      ],
      "metadata": {
        "id": "EkgGjWbKsDxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# VISUALISASI HASIL TRAINING\n",
        "# ============================================\n",
        "\n",
        "def plot_training_history(history, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Fungsi untuk plotting history training\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot accuracy\n",
        "    axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    axes[0].set_title(f'{model_name} - Accuracy')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # Plot loss\n",
        "    axes[1].plot(history.history['loss'], label='Training Loss')\n",
        "    axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
        "    axes[1].set_title(f'{model_name} - Loss')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot history untuk semua model\n",
        "print(\"\\nVisualisasi training history...\")\n",
        "plot_training_history(cnn_history, \"CNN dari Scratch\")\n",
        "plot_training_history(tl_history, \"Transfer Learning\")\n",
        "plot_training_history(finetune_history, \"Fine-tuned Model\")"
      ],
      "metadata": {
        "id": "0xqGNSzFnyGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 14. VISUALISASI PREDIKSI\n",
        "# ============================================\n",
        "\n",
        "def visualize_predictions(images, true_labels, pred_labels, class_names, num_samples=10):\n",
        "    \"\"\"\n",
        "    Fungsi untuk visualisasi prediksi model\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i in range(num_samples):\n",
        "        idx = random.randint(0, len(images)-1)\n",
        "\n",
        "        plt.subplot(2, 5, i+1)\n",
        "        plt.imshow(images[idx])\n",
        "\n",
        "        true_class = class_names[true_labels[idx]]\n",
        "        pred_class = class_names[pred_labels[idx]]\n",
        "\n",
        "        color = 'green' if true_class == pred_class else 'red'\n",
        "\n",
        "        plt.title(f'True: {true_class}\\nPred: {pred_class}', color=color)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle('Contoh Prediksi Model', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualisasi prediksi\n",
        "print(\"\\nVisualisasi contoh prediksi...\")\n",
        "y_true_labels = np.argmax(y_test_onehot, axis=1)\n",
        "\n",
        "# Gunakan CNN predictions\n",
        "visualize_predictions(\n",
        "    x_test[:100],  # Gunakan data asli (belum dinormalisasi untuk visualisasi)\n",
        "    y_true_labels[:100],\n",
        "    cnn_predictions[:100],\n",
        "    class_names\n",
        ")"
      ],
      "metadata": {
        "id": "dxFTGSa3n0H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 15. VISUALISASI CNN LAYERS (FEATURE MAPS)\n",
        "# ============================================\n",
        "\n",
        "def visualize_feature_maps(model, image, layer_names=None):\n",
        "    \"\"\"\n",
        "    Fungsi untuk visualisasi feature maps dari layer CNN\n",
        "    \"\"\"\n",
        "    if layer_names is None:\n",
        "        layer_names = [layer.name for layer in model.layers if 'conv' in layer.name][:3]\n",
        "\n",
        "    # Buat model untuk mendapatkan output dari layer tertentu\n",
        "    outputs = [model.get_layer(name).output for name in layer_names]\n",
        "    vis_model = Model(inputs=model.input, outputs=outputs)\n",
        "\n",
        "    # Predict untuk satu gambar\n",
        "    img_array = np.expand_dims(image, axis=0)\n",
        "    feature_maps = vis_model.predict(img_array)\n",
        "\n",
        "    # Visualisasi feature maps\n",
        "    for layer_name, fmap in zip(layer_names, feature_maps):\n",
        "        print(f\"\\nLayer: {layer_name}, Feature map shape: {fmap.shape}\")\n",
        "\n",
        "        # Ambil beberapa feature maps pertama\n",
        "        num_features = min(8, fmap.shape[-1])\n",
        "\n",
        "        plt.figure(figsize=(15, 2))\n",
        "        plt.suptitle(f'Feature Maps - {layer_name}', fontsize=16)\n",
        "\n",
        "        for i in range(num_features):\n",
        "            plt.subplot(1, num_features, i+1)\n",
        "            plt.imshow(fmap[0, :, :, i], cmap='viridis')\n",
        "            plt.axis('off')\n",
        "            plt.title(f'FM {i+1}')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Pilih satu gambar contoh\n",
        "sample_image = x_test[0]\n",
        "print(\"\\nVisualisasi Feature Maps...\")\n",
        "visualize_feature_maps(cnn_model, sample_image)"
      ],
      "metadata": {
        "id": "PxrSvjYsn1-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **13. Simpan Hasil**"
      ],
      "metadata": {
        "id": "O0x8Tmm4sJRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 16. SIMPAN MODEL DAN HASIL\n",
        "# ============================================\n",
        "\n",
        "def save_models_and_results(cnn_model, tl_model, cnn_accuracy, tl_accuracy):\n",
        "    \"\"\"\n",
        "    Fungsi untuk menyimpan model dan hasil\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MENYIMPAN MODEL DAN HASIL\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Simpan model\n",
        "    cnn_model.save('cnn_fish_classifier.h5')\n",
        "    tl_model.save('transfer_learning_fish_classifier.h5')\n",
        "\n",
        "    print(\"Model CNN disimpan sebagai: cnn_fish_classifier.h5\")\n",
        "    print(\"Model Transfer Learning disimpan sebagai: transfer_learning_fish_classifier.h5\")\n",
        "\n",
        "    # Simpan hasil evaluasi\n",
        "    results = {\n",
        "        'CNN_Accuracy': cnn_accuracy,\n",
        "        'Transfer_Learning_Accuracy': tl_accuracy,\n",
        "        'Num_Classes': num_classes,\n",
        "        'Input_Shape': x_train_split.shape[1:]\n",
        "    }\n",
        "\n",
        "    import json\n",
        "    with open('training_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    print(\"Hasil evaluasi disimpan sebagai: training_results.json\")\n",
        "\n",
        "    # Tampilkan perbandingan akurasi\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    models = ['CNN dari Scratch', 'Transfer Learning']\n",
        "    accuracies = [cnn_accuracy, tl_accuracy]\n",
        "\n",
        "    bars = plt.bar(models, accuracies, color=['skyblue', 'lightgreen'])\n",
        "    plt.title('Perbandingan Akurasi Model')\n",
        "    plt.ylabel('Akurasi')\n",
        "    plt.ylim([0, 1])\n",
        "\n",
        "    # Tambahkan nilai pada bar\n",
        "    for bar, acc in zip(bars, accuracies):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{acc:.4f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Simpan model dan hasil\n",
        "save_models_and_results(cnn_model, tl_model, cnn_accuracy, tl_accuracy)"
      ],
      "metadata": {
        "id": "YxaMTxcKn4I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 17. FUNGSI UNTUK PREDIKSI BARU\n",
        "# ============================================\n",
        "\n",
        "def predict_new_image(model, image_path, class_names, img_size=(32, 32)):\n",
        "    \"\"\"\n",
        "    Fungsi untuk melakukan prediksi pada gambar baru\n",
        "    \"\"\"\n",
        "    # Load dan preprocess gambar\n",
        "    img = Image.open(image_path)\n",
        "    img = img.resize(img_size)\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    # Jika gambar grayscale, konversi ke RGB\n",
        "    if len(img_array.shape) == 2:\n",
        "        img_array = np.stack([img_array]*3, axis=-1)\n",
        "\n",
        "    # Normalisasi\n",
        "    img_array = img_array.astype('float32') / 255.0\n",
        "\n",
        "    # Tambahkan batch dimension\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Prediksi\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = np.argmax(predictions[0])\n",
        "    confidence = np.max(predictions[0])\n",
        "\n",
        "    # Tampilkan hasil\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f'Gambar Input')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.bar(range(len(predictions[0])), predictions[0])\n",
        "    plt.title(f'Prediksi: {class_names[predicted_class]} ({confidence:.2%})')\n",
        "    plt.xlabel('Kelas')\n",
        "    plt.ylabel('Probabilitas')\n",
        "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return predicted_class, confidence, predictions[0]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"IMPLEMENTASI SELESAI\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nPipeline End-to-End untuk klasifikasi gambar telah selesai!\")\n",
        "print(\"Model telah dilatih dan dievaluasi dengan metrik yang sesuai.\")\n",
        "print(\"\\nFitur yang telah diimplementasikan:\")\n",
        "print(\"1. Eksplorasi dan visualisasi data\")\n",
        "print(\"2. Preprocessing dan augmentasi data\")\n",
        "print(\"3. Handling class imbalance\")\n",
        "print(\"4. CNN dari scratch\")\n",
        "print(\"5. Transfer learning dengan VGG16\")\n",
        "print(\"6. Fine-tuning model\")\n",
        "print(\"7. Evaluasi dengan berbagai metrik\")\n",
        "print(\"8. Visualisasi feature maps\")\n",
        "print(\"9. Prediksi pada gambar baru\")"
      ],
      "metadata": {
        "id": "D3uSa55vm7ZM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}