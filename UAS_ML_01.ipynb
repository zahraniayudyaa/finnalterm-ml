{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahraniayudyaa/finnalterm-ml/blob/main/UAS_ML_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8lcx9pa7gQq",
        "outputId": "6f8f79a1-83f8-46b6-93e8-42e5086b95e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost==2.0.3\n",
            "  Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (1.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost==2.0.3) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost==2.0.3) (1.16.3)\n",
            "Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 3.1.2\n",
            "    Uninstalling xgboost-3.1.2:\n",
            "      Successfully uninstalled xgboost-3.1.2\n",
            "Successfully installed xgboost-2.0.3\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost==2.0.3 polars\n",
        "!pip install optuna\n",
        "!pip install polars scikit-learn tensorflow imbalanced-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LANGKAH 1: INSTALL DAN IMPORT POLARS\n",
        "# ============================================\n",
        "\n",
        "# Install polars jika belum ada\n",
        "!pip install polars scikit-learn xgboost tensorflow imbalanced-learn -q\n",
        "\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import polars.selectors as cs\n",
        "from datetime import datetime\n",
        "import gc\n",
        "\n",
        "# Untuk modeling\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
        "\n",
        "# Untuk Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "\n",
        "# Untuk imbalance\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "29bmOkhZ9MQS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LANGKAH 2: LOAD DATA\n",
        "# ============================================\n",
        "\n",
        "!pip install -q gdown\n",
        "\n",
        "# kalian download semua file dari gdrive dibawah lalu buat folder (ex. midterm_folder)\n",
        "!gdown --folder 1qoyfQ0yXp9pgnp9zKwsRm0B-u0n-KopN -O midterm_folder\n",
        "\n",
        "BASE_PATH = \"midterm_folder\"\n",
        "\n",
        "train_df = pl.read_csv(f\"{BASE_PATH}/train_transaction.csv\")\n",
        "test_df = pl.read_csv(f\"{BASE_PATH}/test_transaction.csv\")\n",
        "\n",
        "# print(train_transaction.shape) #590540 row dan 393 feature + 1 target\n",
        "# print(test_transaction.shape) #506691 row dan 393 feature"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEN3LVIt9oK5",
        "outputId": "1a09ecea-e2d0-425d-e705-e0ca3671e055"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1FlTQDZijxi_AdhdPlxvcgKdSRlcbVrKk test_transaction.csv\n",
            "Processing file 18BxWO8J8QgLfWeFInaLnswSph4yNg2RL train_transaction.csv\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1FlTQDZijxi_AdhdPlxvcgKdSRlcbVrKk\n",
            "From (redirected): https://drive.google.com/uc?id=1FlTQDZijxi_AdhdPlxvcgKdSRlcbVrKk&confirm=t&uuid=814daefc-cdf8-4d56-b62a-b351593a41a5\n",
            "To: /content/midterm_folder/test_transaction.csv\n",
            "100% 613M/613M [00:07<00:00, 81.1MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=18BxWO8J8QgLfWeFInaLnswSph4yNg2RL\n",
            "From (redirected): https://drive.google.com/uc?id=18BxWO8J8QgLfWeFInaLnswSph4yNg2RL&confirm=t&uuid=6c2b6bdb-7d01-4551-90e6-475b69a5db9e\n",
            "To: /content/midterm_folder/train_transaction.csv\n",
            "100% 683M/683M [00:08<00:00, 78.2MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head() #isFraud adalah target featurenya\n",
        "test_df.head() #uji data baru, setelah pembuatan model machine learningnya\n",
        "\n",
        "# Cek target distribution\n",
        "print(\"\\nüéØ Distribusi Target (isFraud):\")\n",
        "target_dist = train_df.get_column(\"isFraud\").value_counts().sort(\"isFraud\")\n",
        "print(target_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvV5qKmNLZMz",
        "outputId": "85d67bdb-8608-4473-9639-457b6f9ede27"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ Distribusi Target (isFraud):\n",
            "shape: (2, 2)\n",
            "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ isFraud ‚îÜ count  ‚îÇ\n",
            "‚îÇ ---     ‚îÜ ---    ‚îÇ\n",
            "‚îÇ i64     ‚îÜ u32    ‚îÇ\n",
            "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
            "‚îÇ 0       ‚îÜ 569877 ‚îÇ\n",
            "‚îÇ 1       ‚îÜ 20663  ‚îÇ\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LANGKAH 3: DATA PREPROCESSING DENGAN POLARS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATA PREPROCESSING DENGAN POLARS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "def preprocess_with_polars(df, is_train=True):\n",
        "    \"\"\"\n",
        "    Preprocessing menggunakan Polars\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîß Preprocessing {'train' if is_train else 'test'} data...\")\n",
        "\n",
        "    # Buat copy\n",
        "    df_processed = df.clone()\n",
        "\n",
        "    # 1. Simpan TransactionID untuk submission\n",
        "    if 'TransactionID' in df_processed.columns:\n",
        "        transaction_ids = df_processed.select('TransactionID')\n",
        "        df_processed = df_processed.drop('TransactionID')\n",
        "    else:\n",
        "        transaction_ids = None\n",
        "\n",
        "    # 2. Identifikasi kolom numerik dan kategorikal\n",
        "    numeric_cols = [col for col in df_processed.columns\n",
        "                    if df_processed[col].dtype in [pl.Float64, pl.Float32, pl.Int64, pl.Int32]]\n",
        "    categorical_cols = [col for col in df_processed.columns\n",
        "                       if df_processed[col].dtype == pl.Utf8]\n",
        "\n",
        "    if is_train and 'isFraud' in numeric_cols:\n",
        "        numeric_cols.remove('isFraud')\n",
        "\n",
        "    print(f\"   ‚Ä¢ Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"   ‚Ä¢ Categorical columns: {len(categorical_cols)}\")\n",
        "\n",
        "    # 3. Handle missing values untuk numerik (fill with median)\n",
        "    print(\"   ‚Ä¢ Handling missing values...\")\n",
        "    for col in numeric_cols:\n",
        "        if df_processed[col].null_count() > 0:\n",
        "            median_val = df_processed[col].median()\n",
        "            df_processed = df_processed.with_columns(\n",
        "                pl.col(col).fill_null(median_val)\n",
        "            )\n",
        "\n",
        "    # 4. Handle missing values untuk kategorikal (fill with 'missing')\n",
        "    for col in categorical_cols:\n",
        "        if df_processed[col].null_count() > 0:\n",
        "            df_processed = df_processed.with_columns(\n",
        "                pl.col(col).fill_null('missing')\n",
        "            )\n",
        "\n",
        "    # 5. Encoding kategorikal dengan Label Encoding\n",
        "    print(\"   ‚Ä¢ Encoding categorical features...\")\n",
        "    for col in categorical_cols:\n",
        "        # Corrected: Use cast(pl.Categorical).to_physical() for robust label encoding\n",
        "        df_processed = df_processed.with_columns(\n",
        "            pl.col(col)\n",
        "            .cast(pl.Categorical)\n",
        "            .to_physical()\n",
        "            .alias(f\"{col}_encoded\")\n",
        "        )\n",
        "        df_processed = df_processed.drop(col)\n",
        "\n",
        "    print(f\"‚úÖ Preprocessing selesai. Shape: {df_processed.shape}\")\n",
        "\n",
        "    if is_train:\n",
        "        X = df_processed.drop('isFraud')\n",
        "        y = df_processed.select('isFraud')\n",
        "        return X, y, transaction_ids\n",
        "    else:\n",
        "        return df_processed, transaction_ids\n",
        "\n",
        "# Preprocess train data\n",
        "X_train_pl, y_train_pl, train_ids_pl = preprocess_with_polars(train_df, is_train=True)\n",
        "\n",
        "# Preprocess test data\n",
        "X_test_pl, test_ids_pl = preprocess_with_polars(test_df, is_train=False)\n",
        "\n",
        "# Convert ke numpy untuk modeling\n",
        "print(\"\\nüîÑ Converting Polars DataFrame ke numpy arrays...\")\n",
        "X_train_np = X_train_pl.to_numpy()\n",
        "y_train_np = y_train_pl.to_numpy().flatten()\n",
        "X_test_np = X_test_pl.to_numpy()\n",
        "\n",
        "print(f\"‚úÖ Konversi selesai:\")\n",
        "print(f\"   X_train: {X_train_np.shape}\")\n",
        "print(f\"   y_train: {y_train_np.shape}\")\n",
        "print(f\"   X_test: {X_test_np.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPa4AmoL9xoR",
        "outputId": "492a8e5a-2db2-4af9-a0e9-f1e459d59116"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "DATA PREPROCESSING DENGAN POLARS\n",
            "==================================================\n",
            "\n",
            "üîß Preprocessing train data...\n",
            "   ‚Ä¢ Numeric columns: 378\n",
            "   ‚Ä¢ Categorical columns: 14\n",
            "   ‚Ä¢ Handling missing values...\n",
            "   ‚Ä¢ Encoding categorical features...\n",
            "‚úÖ Preprocessing selesai. Shape: (590540, 393)\n",
            "\n",
            "üîß Preprocessing test data...\n",
            "   ‚Ä¢ Numeric columns: 378\n",
            "   ‚Ä¢ Categorical columns: 14\n",
            "   ‚Ä¢ Handling missing values...\n",
            "   ‚Ä¢ Encoding categorical features...\n",
            "‚úÖ Preprocessing selesai. Shape: (506691, 392)\n",
            "\n",
            "üîÑ Converting Polars DataFrame ke numpy arrays...\n",
            "‚úÖ Konversi selesai:\n",
            "   X_train: (590540, 392)\n",
            "   y_train: (590540,)\n",
            "   X_test: (506691, 392)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LANGKAH 4: SIMPLE FEATURE ENGINEERING (FIXED)\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SIMPLE FEATURE ENGINEERING - NO CRASH\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "def safe_feature_engineering(df):\n",
        "    \"\"\"\n",
        "    Feature engineering yang aman tanpa numpy\n",
        "    \"\"\"\n",
        "    print(\"üß† Creating simple features...\")\n",
        "\n",
        "    df_fe = df.clone()\n",
        "    original_cols = df_fe.columns\n",
        "\n",
        "    # 1. Basic statistics per row\n",
        "    print(\"   ‚Ä¢ Creating row statistics...\")\n",
        "\n",
        "    # Sum per row\n",
        "    sum_expr = pl.sum_horizontal(original_cols)\n",
        "    df_fe = df_fe.with_columns([\n",
        "        sum_expr.alias(\"row_sum\")\n",
        "    ])\n",
        "\n",
        "    # Mean per row\n",
        "    mean_expr = pl.mean_horizontal(original_cols)\n",
        "    df_fe = df_fe.with_columns([\n",
        "        mean_expr.alias(\"row_mean\")\n",
        "    ])\n",
        "\n",
        "    # 2. Cari kolom amount jika ada\n",
        "    amount_cols = [col for col in original_cols if 'amount' in col.lower() or 'amt' in col.lower()]\n",
        "    if amount_cols:\n",
        "        amount_col = amount_cols[0]\n",
        "        print(f\"   ‚Ä¢ Creating amount features from {amount_col}\")\n",
        "\n",
        "        # Pastikan tipe data numeric\n",
        "        if df_fe[amount_col].dtype in [pl.Float64, pl.Float32, pl.Int64, pl.Int32]:\n",
        "            # Standardize amount\n",
        "            mean_val = df_fe[amount_col].mean()\n",
        "            std_val = df_fe[amount_col].std()\n",
        "\n",
        "            if std_val > 0:\n",
        "                df_fe = df_fe.with_columns([\n",
        "                    ((pl.col(amount_col) - mean_val) / std_val).alias(\"amount_standardized\")\n",
        "                ])\n",
        "\n",
        "    # 3. Cari kolom time jika ada\n",
        "    time_cols = [col for col in original_cols if any(keyword in col.lower()\n",
        "                                                     for keyword in ['time', 'dt', 'date'])]\n",
        "\n",
        "    if time_cols:\n",
        "        time_col = time_cols[0]\n",
        "        print(f\"   ‚Ä¢ Creating time features from {time_col}\")\n",
        "\n",
        "        if df_fe[time_col].dtype in [pl.Float64, pl.Float32, pl.Int64, pl.Int32]:\n",
        "            # Simple time features\n",
        "            df_fe = df_fe.with_columns([\n",
        "                (pl.col(time_col) % 24).alias(\"time_hour\"),\n",
        "                ((pl.col(time_col) // 24) % 7).alias(\"time_dayofweek\")\n",
        "            ])\n",
        "\n",
        "    # 4. Interaction feature sederhana\n",
        "    if amount_cols and time_cols:\n",
        "        amount_col = amount_cols[0]\n",
        "        time_col = time_cols[0]\n",
        "\n",
        "        df_fe = df_fe.with_columns([\n",
        "            (pl.col(amount_col) / (pl.col(time_col).abs() + 1)).alias(\"amount_per_time\")\n",
        "        ])\n",
        "\n",
        "    print(f\"‚úÖ Feature engineering completed.\")\n",
        "    print(f\"   Original features: {len(original_cols)}\")\n",
        "    print(f\"   New features: {len(df_fe.columns)}\")\n",
        "\n",
        "    return df_fe\n",
        "\n",
        "# Apply feature engineering dengan error handling\n",
        "print(\"\\nüöÄ Applying feature engineering to train data...\")\n",
        "try:\n",
        "    X_train_fe_pl = safe_feature_engineering(X_train_pl)\n",
        "    print(\"‚úÖ Train feature engineering successful\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error in train feature engineering: {e}\")\n",
        "    print(\"‚ö†Ô∏è  Using original data without feature engineering\")\n",
        "    X_train_fe_pl = X_train_pl\n",
        "\n",
        "print(\"\\nüöÄ Applying feature engineering to test data...\")\n",
        "try:\n",
        "    X_test_fe_pl = safe_feature_engineering(X_test_pl)\n",
        "    print(\"‚úÖ Test feature engineering successful\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error in test feature engineering: {e}\")\n",
        "    print(\"‚ö†Ô∏è  Using original data without feature engineering\")\n",
        "    X_test_fe_pl = X_test_pl\n",
        "\n",
        "print(f\"\\nüìä Final feature shapes:\")\n",
        "print(f\"   X_train_fe: {X_train_fe_pl.shape}\")\n",
        "print(f\"   X_test_fe: {X_test_fe_pl.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCVw5aP292DX",
        "outputId": "4dab7074-de42-4a5e-d1bf-b0cd2b572108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "SIMPLE FEATURE ENGINEERING - NO CRASH\n",
            "==================================================\n",
            "\n",
            "üöÄ Applying feature engineering to train data...\n",
            "üß† Creating simple features...\n",
            "   ‚Ä¢ Creating row statistics...\n",
            "   ‚Ä¢ Creating amount features from TransactionAmt\n",
            "   ‚Ä¢ Creating time features from TransactionDT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LANGKAH 5: TRAIN-TEST SPLIT DAN SCALING\n",
        "# ============================================\n",
        "\n",
        "# Split data jika ada target\n",
        "if y_train_np is not None:\n",
        "    print(\"\\nüìä Splitting data...\")\n",
        "\n",
        "    # Gunakan sample yang lebih kecil untuk hemat memory\n",
        "    SAMPLE_SIZE = min(20000, len(X_train_np))\n",
        "    sample_indices = np.random.choice(len(X_train_np), size=SAMPLE_SIZE, replace=False)\n",
        "\n",
        "    X_sample = X_train_np[sample_indices]\n",
        "    y_sample = y_train_np[sample_indices]\n",
        "\n",
        "    # Split\n",
        "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "        X_sample, y_sample,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y_sample\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Data split completed (using {SAMPLE_SIZE:,} samples):\")\n",
        "    print(f\"   X_train_split: {X_train_split.shape}\")\n",
        "    print(f\"   X_val: {X_val.shape}\")\n",
        "    print(f\"   y_train_split: {y_train_split.shape}\")\n",
        "    print(f\"   y_val: {y_val.shape}\")\n",
        "\n",
        "    # Check class distribution\n",
        "    fraud_rate = y_train_split.mean() * 100\n",
        "    print(f\"‚ö†Ô∏è  Class imbalance: {fraud_rate:.2f}% fraud cases\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No target data available for splitting\")\n",
        "    X_train_split, X_val, y_train_split, y_val = None, None, None, None\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FEATURE SCALING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if X_train_split is not None:\n",
        "    print(\"üìè Scaling features...\")\n",
        "\n",
        "    # Standard scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_split)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test_np)\n",
        "\n",
        "    print(\"‚úÖ Scaling completed\")\n",
        "    print(f\"   X_train_scaled: {X_train_scaled.shape}\")\n",
        "    print(f\"   X_val_scaled: {X_val_scaled.shape}\")\n",
        "    print(f\"   X_test_scaled: {X_test_scaled.shape}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Cannot scale - no training data\")\n",
        "    X_train_scaled, X_val_scaled, X_test_scaled = None, None, None"
      ],
      "metadata": {
        "id": "aHM97oiq94cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LANGKAH 6: DEEP LEARNING MODEL\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DEEP LEARNING MODEL ARCHITECTURE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Clear any existing TensorFlow sessions\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Configure GPU memory growth\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"‚úÖ GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"‚ö†Ô∏è  GPU configuration error: {e}\")\n",
        "\n",
        "if X_train_scaled is not None:\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "\n",
        "    print(f\"\\nüèóÔ∏è  Building Neural Network...\")\n",
        "    print(f\"   ‚Ä¢ Input dimension: {input_dim}\")\n",
        "    print(f\"   ‚Ä¢ Training samples: {X_train_scaled.shape[0]:,}\")\n",
        "\n",
        "    def build_fraud_detection_model(input_dim, learning_rate=0.001):\n",
        "        \"\"\"Build a simple but effective neural network for fraud detection\"\"\"\n",
        "\n",
        "        model = keras.Sequential([\n",
        "            # Input layer\n",
        "            layers.Input(shape=(input_dim,)),\n",
        "\n",
        "            # First hidden layer with batch normalization\n",
        "            layers.Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.3),\n",
        "\n",
        "            # Second hidden layer\n",
        "            layers.Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.2),\n",
        "\n",
        "            # Third hidden layer\n",
        "            layers.Dense(16, activation='relu', kernel_initializer='he_normal'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.1),\n",
        "\n",
        "            # Output layer\n",
        "            layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = keras.optimizers.Adam(\n",
        "            learning_rate=learning_rate,\n",
        "            beta_1=0.9,\n",
        "            beta_2=0.999,\n",
        "            epsilon=1e-7\n",
        "        )\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=[\n",
        "                'accuracy',\n",
        "                keras.metrics.AUC(name='auc'),\n",
        "                keras.metrics.Precision(name='precision'),\n",
        "                keras.metrics.Recall(name='recall')\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    # Build model\n",
        "    dl_model = build_fraud_detection_model(input_dim, learning_rate=0.001)\n",
        "\n",
        "    print(\"\\nüìã Model Summary:\")\n",
        "    dl_model.summary()\n",
        "\n",
        "    # Calculate model size\n",
        "    trainable_params = dl_model.count_params()\n",
        "    print(f\"\\nüìä Model Statistics:\")\n",
        "    print(f\"   ‚Ä¢ Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"   ‚Ä¢ Model size: ~{trainable_params * 4 / (1024**2):.2f} MB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Cannot build model - no training data available\")\n",
        "    dl_model = None"
      ],
      "metadata": {
        "id": "xd-38eEfKpge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LANGKAH 6.5: BASIC HYPERPARAMETER TUNING DEEP LEARNING\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BASIC HYPERPARAMETER TUNING - DEEP LEARNING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if X_train_scaled is not None:\n",
        "    print(\"üîß Testing different neural network architectures...\")\n",
        "\n",
        "    # Define different configurations to test\n",
        "    configs = [\n",
        "        # Config 1: Small network\n",
        "        {\n",
        "            'name': 'Small_Network',\n",
        "            'layers': [32, 16],  # 2 layers\n",
        "            'dropouts': [0.2, 0.1],\n",
        "            'learning_rate': 0.001,\n",
        "            'batch_size': 64\n",
        "        },\n",
        "        # Config 2: Medium network (default)\n",
        "        {\n",
        "            'name': 'Medium_Network',\n",
        "            'layers': [64, 32, 16],  # 3 layers\n",
        "            'dropouts': [0.3, 0.2, 0.1],\n",
        "            'learning_rate': 0.001,\n",
        "            'batch_size': 128\n",
        "        },\n",
        "        # Config 3: Deeper network\n",
        "        {\n",
        "            'name': 'Deep_Network',\n",
        "            'layers': [128, 64, 32, 16],  # 4 layers\n",
        "            'dropouts': [0.4, 0.3, 0.2, 0.1],\n",
        "            'learning_rate': 0.0005,\n",
        "            'batch_size': 256\n",
        "        },\n",
        "        # Config 4: Wide network\n",
        "        {\n",
        "            'name': 'Wide_Network',\n",
        "            'layers': [128, 64],  # 2 wide layers\n",
        "            'dropouts': [0.3, 0.2],\n",
        "            'learning_rate': 0.01,\n",
        "            'batch_size': 64\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Function to build model from config\n",
        "    def build_model_from_config(input_dim, config):\n",
        "        \"\"\"Build model based on configuration\"\"\"\n",
        "        model = keras.Sequential()\n",
        "        model.add(layers.Input(shape=(input_dim,)))\n",
        "\n",
        "        # Add hidden layers\n",
        "        for i, (neurons, dropout) in enumerate(zip(config['layers'], config['dropouts'])):\n",
        "            model.add(layers.Dense(neurons, activation='relu', kernel_initializer='he_normal'))\n",
        "            model.add(layers.BatchNormalization())\n",
        "            model.add(layers.Dropout(dropout))\n",
        "\n",
        "        # Output layer\n",
        "        model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "        # Compile\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=[\n",
        "                'accuracy',\n",
        "                keras.metrics.AUC(name='auc'),\n",
        "                keras.metrics.Precision(name='precision'), # Added precision\n",
        "                keras.metrics.Recall(name='recall')    # Added recall\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    # Train and evaluate each configuration\n",
        "    tuning_results = []\n",
        "    best_val_auc = 0\n",
        "    best_config = None\n",
        "    best_model = None\n",
        "\n",
        "    print(f\"\\nüéØ Testing {len(configs)} different configurations...\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, config in enumerate(configs):\n",
        "        print(f\"\\nüî¨ Testing Configuration {i+1}: {config['name']}\")\n",
        "        print(f\"   ‚Ä¢ Architecture: {config['layers']}\")\n",
        "        print(f\"   ‚Ä¢ Dropouts: {config['dropouts']}\")\n",
        "        print(f\"   ‚Ä¢ Learning rate: {config['learning_rate']}\")\n",
        "        print(f\"   ‚Ä¢ Batch size: {config['batch_size']}\")\n",
        "\n",
        "        # Build model\n",
        "        model = build_model_from_config(input_dim, config)\n",
        "\n",
        "        # Train for a few epochs to evaluate\n",
        "        print(f\"   ‚Ä¢ Training for 5 epochs...\")\n",
        "        history = model.fit(\n",
        "            X_train_scaled,\n",
        "            y_train_split,\n",
        "            batch_size=config['batch_size'],\n",
        "            epochs=5,  # Short training for tuning\n",
        "            validation_data=(X_val_scaled, y_val),\n",
        "            verbose=0,\n",
        "            class_weight=class_weight_dict\n",
        "        )\n",
        "\n",
        "        # Get validation AUC\n",
        "        val_auc = history.history['val_auc'][-1]\n",
        "        train_auc = history.history['auc'][-1]\n",
        "\n",
        "        tuning_results.append({\n",
        "            'config_name': config['name'],\n",
        "            'config': config,\n",
        "            'val_auc': val_auc,\n",
        "            'train_auc': train_auc,\n",
        "            'model': model\n",
        "        })\n",
        "\n",
        "        print(f\"   ‚úÖ Training AUC: {train_auc:.4f}\")\n",
        "        print(f\"   ‚úÖ Validation AUC: {val_auc:.4f}\")\n",
        "\n",
        "        # Update best model\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "            best_config = config\n",
        "            best_model = model\n",
        "            print(f\"   üéØ NEW BEST CONFIGURATION!\")\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"HYPERPARAMETER TUNING RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Sort results by validation AUC\n",
        "    tuning_results.sort(key=lambda x: x['val_auc'], reverse=True)\n",
        "\n",
        "    for i, result in enumerate(tuning_results):\n",
        "        rank = i + 1\n",
        "        print(f\"{rank}. {result['config_name']:20} Val AUC: {result['val_auc']:.4f} | Train AUC: {result['train_auc']:.4f}\")\n",
        "\n",
        "    print(f\"\\nüèÜ BEST CONFIGURATION: {best_config['name']}\")\n",
        "    print(f\"üèÜ BEST VALIDATION AUC: {best_val_auc:.4f}\")\n",
        "    print(f\"\\nüìã Best Configuration Details:\")\n",
        "    print(f\"   ‚Ä¢ Layers: {best_config['layers']}\")\n",
        "    print(f\"   ‚Ä¢ Dropouts: {best_config['dropouts']}\")\n",
        "    print(f\"   ‚Ä¢ Learning rate: {best_config['learning_rate']}\")\n",
        "    print(f\"   ‚Ä¢ Batch size: {best_config['batch_size']}\")\n",
        "\n",
        "    # Use best model for full training\n",
        "    dl_model = best_model\n",
        "    print(\"\\n‚úÖ Using best configuration for full training\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Cannot perform hyperparameter tuning - no training data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "upiT4kGnSutF",
        "outputId": "d2511b52-e497-41dc-bb47-f64a4cb58b26"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "BASIC HYPERPARAMETER TUNING - DEEP LEARNING\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_scaled' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3776751285.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mX_train_scaled\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üîß Testing different neural network architectures...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LANGKAH 7: CALLBACKS AND CLASS WEIGHTS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING CONFIGURATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if y_train_split is not None and dl_model is not None:\n",
        "    # Calculate class weights\n",
        "    print(\"\\n‚öñÔ∏è  Calculating class weights...\")\n",
        "\n",
        "    from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "    classes = np.unique(y_train_split)\n",
        "    class_weights = compute_class_weight(\n",
        "        'balanced',\n",
        "        classes=classes,\n",
        "        y=y_train_split\n",
        "    )\n",
        "\n",
        "    class_weight_dict = dict(zip(classes, class_weights))\n",
        "    print(f\"   ‚Ä¢ Class weights: {class_weight_dict}\")\n",
        "\n",
        "    # Setup callbacks\n",
        "    print(\"\\n‚öôÔ∏è  Setting up callbacks...\")\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = callbacks.EarlyStopping(\n",
        "        monitor='val_auc',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Reduce learning rate\n",
        "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Model checkpoint\n",
        "    checkpoint = callbacks.ModelCheckpoint(\n",
        "        'best_dl_model.keras',\n",
        "        monitor='val_auc',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    callbacks_list = [early_stopping, reduce_lr, checkpoint]\n",
        "\n",
        "    print(\"‚úÖ Callbacks configured\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Cannot setup training configuration\")\n",
        "    callbacks_list = []\n",
        "    class_weight_dict = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "ylwNH-ynQ9Nk",
        "outputId": "cc878f24-0e73-4dd8-a20d-cf7cfdea3ce8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "TRAINING CONFIGURATION\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_train_split' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2658275847.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0my_train_split\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdl_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Calculate class weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n‚öñÔ∏è  Calculating class weights...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train_split' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LANGKAH 8: TRAIN DEEP LEARNING MODEL\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING DEEP LEARNING MODEL\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if X_train_scaled is not None and y_train_split is not None and dl_model is not None:\n",
        "    print(\"üöÄ Starting training...\")\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 128  # Small batch size for memory efficiency\n",
        "    EPOCHS = 25\n",
        "\n",
        "    print(f\"   ‚Ä¢ Batch size: {BATCH_SIZE}\")\n",
        "    print(f\"   ‚Ä¢ Max epochs: {EPOCHS}\")\n",
        "    print(f\"   ‚Ä¢ Training samples: {X_train_scaled.shape[0]:,}\")\n",
        "    print(f\"   ‚Ä¢ Validation samples: {X_val_scaled.shape[0]:,}\")\n",
        "\n",
        "    # Train model\n",
        "    history = dl_model.fit(\n",
        "        X_train_scaled,\n",
        "        y_train_split,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=(X_val_scaled, y_val),\n",
        "        callbacks=callbacks_list,\n",
        "        class_weight=class_weight_dict,\n",
        "        verbose=1,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ Training completed!\")\n",
        "\n",
        "    # Plot training history\n",
        "    print(\"\\nüìà Plotting training history...\")\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "    # Plot loss\n",
        "    axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
        "    axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
        "    axes[0, 0].set_title('Loss over Epochs')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot AUC\n",
        "    axes[0, 1].plot(history.history['auc'], label='Training AUC')\n",
        "    axes[0, 1].plot(history.history['val_auc'], label='Validation AUC')\n",
        "    axes[0, 1].set_title('AUC over Epochs')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('AUC')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot accuracy\n",
        "    axes[1, 0].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    axes[1, 0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    axes[1, 0].set_title('Accuracy over Epochs')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Accuracy')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot precision and recall\n",
        "    axes[1, 1].plot(history.history['precision'], label='Training Precision')\n",
        "    axes[1, 1].plot(history.history['val_precision'], label='Validation Precision')\n",
        "    axes[1, 1].plot(history.history['recall'], label='Training Recall')\n",
        "    axes[1, 1].plot(history.history['val_recall'], label='Validation Recall')\n",
        "    axes[1, 1].set_title('Precision & Recall over Epochs')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Score')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Cannot train model - insufficient data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "IJAjw6mHRA1s",
        "outputId": "de4c999e-1ded-47f8-dac0-b479292f74bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "TRAINING DEEP LEARNING MODEL\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_scaled' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1118453128.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mX_train_scaled\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my_train_split\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdl_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üöÄ Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LANGKAH 9: EVALUATE MODEL\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if X_val_scaled is not None and y_val is not None:\n",
        "    print(\"üìä Evaluating model on validation set...\")\n",
        "\n",
        "    # Load best model if available\n",
        "    try:\n",
        "        dl_model = keras.models.load_model('best_dl_model.keras')\n",
        "        print(\"‚úÖ Loaded best saved model\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è  Using last trained model\")\n",
        "\n",
        "    # Evaluate\n",
        "    val_results = dl_model.evaluate(X_val_scaled, y_val, verbose=0)\n",
        "\n",
        "    print(f\"\\nüìà Validation Results:\")\n",
        "    print(f\"   ‚Ä¢ Loss: {val_results[0]:.4f}\")\n",
        "    print(f\"   ‚Ä¢ Accuracy: {val_results[1]:.4f}\")\n",
        "    print(f\"   ‚Ä¢ AUC: {val_results[2]:.4f}\")\n",
        "    print(f\"   ‚Ä¢ Precision: {val_results[3]:.4f}\")\n",
        "    print(f\"   ‚Ä¢ Recall: {val_results[4]:.4f}\")\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"\\nüîÆ Making predictions...\")\n",
        "    y_pred_proba = dl_model.predict(X_val_scaled, batch_size=1024, verbose=0).flatten()\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve\n",
        "\n",
        "    # AUC\n",
        "    auc_score = roc_auc_score(y_val, y_pred_proba)\n",
        "    print(f\"   ‚Ä¢ AUC (custom): {auc_score:.4f}\")\n",
        "\n",
        "    # Find optimal threshold\n",
        "    precisions, recalls, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
        "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
        "    optimal_idx = np.argmax(f1_scores)\n",
        "    optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "    print(f\"   ‚Ä¢ Optimal threshold: {optimal_threshold:.4f}\")\n",
        "    print(f\"   ‚Ä¢ Max F1-Score: {f1_scores[optimal_idx]:.4f}\")\n",
        "\n",
        "    # Binary predictions\n",
        "    y_pred_binary = (y_pred_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_val, y_pred_binary)\n",
        "\n",
        "    print(f\"\\nüéØ Confusion Matrix (threshold={optimal_threshold:.3f}):\")\n",
        "    print(f\"   TN: {cm[0,0]:6d} | FP: {cm[0,1]:6d}\")\n",
        "    print(f\"   FN: {cm[1,0]:6d} | TP: {cm[1,1]:6d}\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall = tp / (tp + fn + 1e-8)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "\n",
        "    print(f\"\\nüìä Detailed Metrics:\")\n",
        "    print(f\"   ‚Ä¢ Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"   ‚Ä¢ Precision: {precision:.4f}\")\n",
        "    print(f\"   ‚Ä¢ Recall:    {recall:.4f}\")\n",
        "    print(f\"   ‚Ä¢ F1-Score:  {f1:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Cannot evaluate model - no validation data\")"
      ],
      "metadata": {
        "id": "SoK6JLgWRHim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LANGKAH 10: PREDICT ON TEST DATA\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST DATA PREDICTION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if dl_model is not None and X_test_scaled is not None:\n",
        "    print(\"üîÆ Predicting on test data...\")\n",
        "\n",
        "    # Predict in batches to save memory\n",
        "    def predict_batches(model, X_data, batch_size=2048):\n",
        "        \"\"\"Predict in batches\"\"\"\n",
        "        predictions = []\n",
        "        n_samples = X_data.shape[0]\n",
        "\n",
        "        print(f\"   ‚Ä¢ Predicting {n_samples:,} samples in batches of {batch_size}...\")\n",
        "\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            batch_end = min(i + batch_size, n_samples)\n",
        "            X_batch = X_data[i:batch_end]\n",
        "\n",
        "            # Predict batch\n",
        "            batch_preds = model.predict(X_batch, verbose=0).flatten()\n",
        "            predictions.extend(batch_preds)\n",
        "\n",
        "            # Progress\n",
        "            if (i // batch_size) % 10 == 0:\n",
        "                print(f\"     Processed {batch_end:,}/{n_samples:,} samples\")\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    # Make predictions\n",
        "    test_predictions = predict_batches(dl_model, X_test_scaled, batch_size=4096)\n",
        "\n",
        "    print(f\"\\n‚úÖ Predictions completed:\")\n",
        "    print(f\"   ‚Ä¢ Shape: {test_predictions.shape}\")\n",
        "    print(f\"   ‚Ä¢ Min probability: {test_predictions.min():.4f}\")\n",
        "    print(f\"   ‚Ä¢ Max probability: {test_predictions.max():.4f}\")\n",
        "    print(f\"   ‚Ä¢ Mean probability: {test_predictions.mean():.4f}\")\n",
        "    print(f\"   ‚Ä¢ Std probability: {test_predictions.std():.4f}\")\n",
        "\n",
        "    # Count predictions above threshold\n",
        "    threshold = 0.5\n",
        "    fraud_count = (test_predictions >= threshold).sum()\n",
        "    legit_count = len(test_predictions) - fraud_count\n",
        "\n",
        "    print(f\"\\nüìà Prediction Distribution:\")\n",
        "    print(f\"   ‚Ä¢ Legit (<{threshold}): {legit_count:,} ({legit_count/len(test_predictions)*100:.1f}%)\")\n",
        "    print(f\"   ‚Ä¢ Fraud (‚â•{threshold}): {fraud_count:,} ({fraud_count/len(test_predictions)*100:.1f}%)\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Cannot make predictions - model or test data not available\")\n",
        "    test_predictions = None"
      ],
      "metadata": {
        "id": "1yARAKZBRLn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LANGKAH 11: CREATE SUBMISSION FILE\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CREATE SUBMISSION FILE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n",
        "print(\"üìù Creating submission file...\")\n",
        "\n",
        "# Get test IDs (trim to match predictions if needed)\n",
        "test_ids_list = test_ids_pl['TransactionID'].to_list()\n",
        "\n",
        "# Ensure same length\n",
        "if len(test_ids_list) != len(test_predictions):\n",
        "    print(f\"‚ö†Ô∏è  Length mismatch: IDs={len(test_ids_list)}, Predictions={len(test_predictions)}\")\n",
        "    # Trim to shorter length\n",
        "    min_length = min(len(test_ids_list), len(test_predictions))\n",
        "    test_ids_list = test_ids_list[:min_length]\n",
        "    test_predictions = test_predictions[:min_length]\n",
        "    print(f\"‚úÖ Trimmed to {min_length} samples\")\n",
        "\n",
        "# Create DataFrame\n",
        "submission_df = pl.DataFrame({\n",
        "    'TransactionID': test_ids_list,\n",
        "    'isFraud': test_predictions\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission_file = 'fraud_detection_dl_submission.csv'\n",
        "submission_df.write_csv(submission_file)\n",
        "\n",
        "print(f\"\\n‚úÖ Submission file created: {submission_file}\")\n",
        "print(\"\\nüìä Preview:\")\n",
        "print(submission_df.head())"
      ],
      "metadata": {
        "id": "9W40eckkRT5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LANGKAH 12: SAVE MODEL AND RESULTS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SAVE MODEL AND RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "import json\n",
        "import joblib\n",
        "\n",
        "# Save model\n",
        "if dl_model is not None:\n",
        "    print(\"üíæ Saving model...\")\n",
        "\n",
        "    # Save Keras model\n",
        "    dl_model.save('fraud_detection_dl_model.h5')\n",
        "    print(\"   ‚úÖ Model saved: fraud_detection_dl_model.h5\")\n",
        "\n",
        "    # Save scaler\n",
        "    if 'scaler' in locals():\n",
        "        joblib.dump(scaler, 'scaler.pkl')\n",
        "        print(\"   ‚úÖ Scaler saved: scaler.pkl\")\n",
        "\n",
        "# Save results summary\n",
        "results_summary = {\n",
        "    'model_info': {\n",
        "        'architecture': '3-layer Dense Neural Network',\n",
        "        'input_dim': input_dim if 'input_dim' in locals() else 'unknown',\n",
        "        'parameters': trainable_params if 'trainable_params' in locals() else 'unknown'\n",
        "    },\n",
        "    'performance': {},\n",
        "    'data_info': {\n",
        "        'train_samples': X_train_scaled.shape[0] if X_train_scaled is not None else 'unknown',\n",
        "        'test_samples': X_test_scaled.shape[0] if X_test_scaled is not None else 'unknown',\n",
        "        'features': input_dim if 'input_dim' in locals() else 'unknown'\n",
        "    },\n",
        "    'submission': {\n",
        "        'file': submission_file if 'submission_file' in locals() else 'not_created',\n",
        "        'samples': len(test_predictions) if test_predictions is not None else 0\n",
        "    },\n",
        "    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "}\n",
        "\n",
        "# Add validation metrics if available\n",
        "if 'val_results' in locals():\n",
        "    results_summary['performance'] = {\n",
        "        'loss': float(val_results[0]),\n",
        "        'accuracy': float(val_results[1]),\n",
        "        'auc': float(val_results[2]),\n",
        "        'precision': float(val_results[3]),\n",
        "        'recall': float(val_results[4])\n",
        "    }\n",
        "\n",
        "# Save to JSON\n",
        "with open('dl_results_summary.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=4)\n",
        "\n",
        "print(\"   ‚úÖ Results saved: dl_results_summary.json\")\n",
        "\n",
        "print(f\"\\nüìã FINAL SUMMARY:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Model: Deep Neural Network (3 layers)\")\n",
        "print(f\"Input Features: {results_summary['data_info']['features']}\")\n",
        "print(f\"Train Samples: {results_summary['data_info']['train_samples']:,}\")\n",
        "print(f\"Test Predictions: {results_summary['submission']['samples']:,}\")\n",
        "\n",
        "if 'performance' in results_summary and results_summary['performance']:\n",
        "    print(f\"Validation AUC: {results_summary['performance']['auc']:.4f}\")\n",
        "\n",
        "if 'submission_file' in locals():\n",
        "    print(f\"Submission File: {submission_file}\")"
      ],
      "metadata": {
        "id": "F0R2_3zdR6J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LANGKAH 13: VISUALIZE PREDICTIONS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"VISUALIZE PREDICTIONS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if test_predictions is not None:\n",
        "    print(\"üìä Creating visualization...\")\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Distribution of predictions\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(test_predictions, bins=50, alpha=0.7, color='#2E86AB', edgecolor='black')\n",
        "    plt.title('Distribution of Fraud Probabilities', fontsize=12)\n",
        "    plt.xlabel('Probability of Fraud', fontsize=10)\n",
        "    plt.ylabel('Count', fontsize=10)\n",
        "    plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='Threshold=0.5')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # Pie chart of predictions\n",
        "    plt.subplot(1, 2, 2)\n",
        "\n",
        "    threshold = 0.5\n",
        "    fraud_count = (test_predictions >= threshold).sum()\n",
        "    legit_count = len(test_predictions) - fraud_count\n",
        "\n",
        "    labels = [f'Legit\\n({legit_count:,})', f'Fraud\\n({fraud_count:,})']\n",
        "    sizes = [legit_count, fraud_count]\n",
        "    colors = ['#73D2DE', '#F61067']\n",
        "    explode = (0, 0.1)\n",
        "\n",
        "    plt.pie(sizes, labels=labels, colors=colors, explode=explode,\n",
        "            autopct='%1.1f%%', startangle=90, textprops={'fontsize': 9})\n",
        "    plt.title('Predicted Fraud vs Legit Transactions', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"‚úÖ Visualization completed\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Cannot create visualization - no predictions available\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéâ DEEP LEARNING PIPELINE COMPLETED! üéâ\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\n‚úÖ All steps executed successfully\")\n",
        "print(\"‚úÖ Model trained and evaluated\")\n",
        "print(\"‚úÖ Predictions made on test data\")\n",
        "if 'submission_file' in locals():\n",
        "    print(f\"‚úÖ Submission file created: {submission_file}\")\n",
        "print(\"\\nüìÅ Files created:\")\n",
        "print(\"   1. fraud_detection_dl_model.h5 - Trained model\")\n",
        "print(\"   2. scaler.pkl - Feature scaler\")\n",
        "print(\"   3. dl_results_summary.json - Results summary\")\n",
        "if 'submission_file' in locals():\n",
        "    print(f\"   4. {submission_file} - Submission file\")"
      ],
      "metadata": {
        "id": "zk6bz6qFR9vp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}